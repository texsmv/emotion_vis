{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mts.core.mtserie_dataset import MTSerieDataset\n",
    "from mts.core.projections import ProjectionAlg\n",
    "from models.emotion_dataset_controller import *\n",
    "import matplotlib.pyplot as plt\n",
    "from mts.core.projections import ProjectionAlg, euclidean_distance_matrix, mds_projection, compute_k_distance_matrixes, compute_distance_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config));\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/texs/Documents/AirQuality/repositories/peax/experiments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = AppController()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_id = 'emotions_in_music'\n",
    "dataset_id = 'afew_va'\n",
    "controller.loadLocalDataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 34, 2)\n",
      "0.95\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = controller.datasets[dataset_id]\n",
    "X = dataset.values()\n",
    "\n",
    "print(X.shape)\n",
    "print(X.max())\n",
    "print(X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalizeWindow(data, percentile: float = 99.9):\n",
    "    cutoff = np.percentile(data, (0, percentile))\n",
    "    data_norm = np.copy(data)\n",
    "    data_norm[np.where(data_norm < cutoff[0])] = cutoff[0]\n",
    "    data_norm[np.where(data_norm > cutoff[1])] = cutoff[1]\n",
    "\n",
    "    return MinMaxScaler().fit_transform(data_norm)\n",
    "\n",
    "X = dataset.values()\n",
    "# X = np.array([  normalizeWindow(np.concatenate([x, np.zeros([1, 2])]) )    for x in X])\n",
    "# X = np.array([(np.concatenate([x, np.zeros([1, 2])]) )    for x in X])\n",
    "\n",
    "\n",
    "N, T, D =X.shape\n",
    "allWindows = np.split(X, 2, axis = 2)\n",
    "windows1 = allWindows[0]\n",
    "# print(windows1.shape)\n",
    "# print(windows2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 34, 1)\n",
      "(83, 34, 1)\n",
      "(85, 34, 1)\n"
     ]
    }
   ],
   "source": [
    "windows = windows1\n",
    "n = windows.shape[0]\n",
    "train_index = int(n* 0.6)\n",
    "test_index = train_index + int(n* 0.2)\n",
    "train = windows[0: train_index]\n",
    "test = windows[train_index: test_index]\n",
    "val = windows[test_index: ]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)\n",
    "\n",
    "DATA_FILEPATH = f'savedData/data.npy'\n",
    "with open(DATA_FILEPATH, 'wb') as f:\n",
    "    np.save(f, train)\n",
    "    np.save(f, test)\n",
    "    np.save(f, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"conv_filters\": [\n",
    "    128,\n",
    "    192,\n",
    "    288,\n",
    "    432\n",
    "  ],\n",
    "  \"conv_kernels\": [\n",
    "    3,\n",
    "    5,\n",
    "    7,\n",
    "    9\n",
    "  ],\n",
    "  \"dense_units\": [\n",
    "    1024,\n",
    "    256\n",
    "  ],\n",
    "  \"dropouts\": [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "  ],\n",
    "  \"embedding\": 25,\n",
    "  \"reg_lambda\": 0,\n",
    "  \"optimizer\": \"adadelta\",\n",
    "  \"learning_rate\": 1.0,\n",
    "  \"learning_rate_decay\": 0.001,\n",
    "  \"loss\": \"bce\",\n",
    "  \"metrics\": [],\n",
    "  \"batch_norm\": [\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    False,\n",
    "    False\n",
    "  ],\n",
    "  \"batch_norm_input\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = f'savedData/data.npy'\n",
    "with open(DATA_FILEPATH, 'rb') as f:\n",
    "    train_data = np.load(f, allow_pickle=True)\n",
    "    test_data = np.load(f, allow_pickle=True)\n",
    "    val_data = np.load(f, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn_model import create_model\n",
    "\n",
    "# _, _, model = create_model(input_dim=T, n_features=D, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/texs/Documents/AirQuality/repositories/peax/experiments/ae/loss.py:11: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/mass_ts/_mass_ts.py:18: UserWarning: GPU support will not work. You must pip install mass-ts[gpu].\n",
      "  'GPU support will not work. You must pip install mass-ts[gpu].')\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/texs/anaconda3/envs/peax3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 250 samples, validate on 85 samples\n",
      "Epoch 1/120\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6909 - val_loss: 0.6836\n",
      "Epoch 2/120\n",
      "250/250 [==============================] - 0s 176us/step - loss: 0.6796 - val_loss: 0.6728\n",
      "Epoch 3/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6682 - val_loss: 0.6668\n",
      "Epoch 4/120\n",
      "250/250 [==============================] - 0s 177us/step - loss: 0.6628 - val_loss: 0.6662\n",
      "Epoch 5/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6620 - val_loss: 0.6669\n",
      "Epoch 6/120\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.6621 - val_loss: 0.6663\n",
      "Epoch 7/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6619 - val_loss: 0.6665\n",
      "Epoch 8/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6617 - val_loss: 0.6659\n",
      "Epoch 9/120\n",
      "250/250 [==============================] - 0s 144us/step - loss: 0.6617 - val_loss: 0.6658\n",
      "Epoch 10/120\n",
      "250/250 [==============================] - 0s 163us/step - loss: 0.6616 - val_loss: 0.6658\n",
      "Epoch 11/120\n",
      "250/250 [==============================] - 0s 146us/step - loss: 0.6614 - val_loss: 0.6660\n",
      "Epoch 12/120\n",
      "250/250 [==============================] - 0s 176us/step - loss: 0.6613 - val_loss: 0.6655\n",
      "Epoch 13/120\n",
      "250/250 [==============================] - 0s 159us/step - loss: 0.6612 - val_loss: 0.6655\n",
      "Epoch 14/120\n",
      "250/250 [==============================] - 0s 169us/step - loss: 0.6610 - val_loss: 0.6650\n",
      "Epoch 15/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6610 - val_loss: 0.6663\n",
      "Epoch 16/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6611 - val_loss: 0.6650\n",
      "Epoch 17/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6611 - val_loss: 0.6642\n",
      "Epoch 18/120\n",
      "250/250 [==============================] - 0s 163us/step - loss: 0.6605 - val_loss: 0.6639\n",
      "Epoch 19/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6602 - val_loss: 0.6632\n",
      "Epoch 20/120\n",
      "250/250 [==============================] - 0s 172us/step - loss: 0.6606 - val_loss: 0.6648\n",
      "Epoch 21/120\n",
      "250/250 [==============================] - 0s 156us/step - loss: 0.6605 - val_loss: 0.6623\n",
      "Epoch 22/120\n",
      "250/250 [==============================] - 0s 166us/step - loss: 0.6590 - val_loss: 0.6613\n",
      "Epoch 23/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6584 - val_loss: 0.6609\n",
      "Epoch 24/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6619 - val_loss: 0.6594\n",
      "Epoch 25/120\n",
      "250/250 [==============================] - 0s 163us/step - loss: 0.6585 - val_loss: 0.6596\n",
      "Epoch 26/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6609 - val_loss: 0.6611\n",
      "Epoch 27/120\n",
      "250/250 [==============================] - 0s 169us/step - loss: 0.6584 - val_loss: 0.6546\n",
      "Epoch 28/120\n",
      "250/250 [==============================] - 0s 163us/step - loss: 0.6584 - val_loss: 0.6708\n",
      "Epoch 29/120\n",
      "250/250 [==============================] - 0s 169us/step - loss: 0.6642 - val_loss: 0.6680\n",
      "Epoch 30/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6611 - val_loss: 0.6584\n",
      "Epoch 31/120\n",
      "250/250 [==============================] - 0s 166us/step - loss: 0.6612 - val_loss: 0.6555\n",
      "Epoch 32/120\n",
      "250/250 [==============================] - 0s 163us/step - loss: 0.6590 - val_loss: 0.6527\n",
      "Epoch 33/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6546 - val_loss: 0.6555\n",
      "Epoch 34/120\n",
      "250/250 [==============================] - 0s 166us/step - loss: 0.6583 - val_loss: 0.6572\n",
      "Epoch 35/120\n",
      "250/250 [==============================] - 0s 174us/step - loss: 0.6630 - val_loss: 0.6537\n",
      "Epoch 36/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6559 - val_loss: 0.6551\n",
      "Epoch 37/120\n",
      "250/250 [==============================] - 0s 169us/step - loss: 0.6546 - val_loss: 0.6461\n",
      "Epoch 38/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6506 - val_loss: 0.6626\n",
      "Epoch 39/120\n",
      "250/250 [==============================] - 0s 156us/step - loss: 0.6604 - val_loss: 0.6455\n",
      "Epoch 40/120\n",
      "250/250 [==============================] - 0s 147us/step - loss: 0.6539 - val_loss: 0.6716\n",
      "Epoch 41/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6626 - val_loss: 0.6439\n",
      "Epoch 42/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6575 - val_loss: 0.6774\n",
      "Epoch 43/120\n",
      "250/250 [==============================] - 0s 147us/step - loss: 0.6730 - val_loss: 0.6693\n",
      "Epoch 44/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6650 - val_loss: 0.6664\n",
      "Epoch 45/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6621 - val_loss: 0.6662\n",
      "Epoch 46/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6617 - val_loss: 0.6663\n",
      "Epoch 47/120\n",
      "250/250 [==============================] - 0s 145us/step - loss: 0.6617 - val_loss: 0.6662\n",
      "Epoch 48/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6616 - val_loss: 0.6666\n",
      "Epoch 49/120\n",
      "250/250 [==============================] - 0s 173us/step - loss: 0.6616 - val_loss: 0.6663\n",
      "Epoch 50/120\n",
      "250/250 [==============================] - 0s 165us/step - loss: 0.6615 - val_loss: 0.6663\n",
      "Epoch 51/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6616 - val_loss: 0.6666\n",
      "Epoch 52/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6615 - val_loss: 0.6662\n",
      "Epoch 53/120\n",
      "250/250 [==============================] - 0s 170us/step - loss: 0.6615 - val_loss: 0.6664\n",
      "Epoch 54/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6614 - val_loss: 0.6662\n",
      "Epoch 55/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6615 - val_loss: 0.6661\n",
      "Epoch 56/120\n",
      "250/250 [==============================] - 0s 165us/step - loss: 0.6616 - val_loss: 0.6661\n",
      "Epoch 57/120\n",
      "250/250 [==============================] - 0s 178us/step - loss: 0.6615 - val_loss: 0.6660\n",
      "Epoch 58/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6613 - val_loss: 0.6660\n",
      "Epoch 59/120\n",
      "250/250 [==============================] - 0s 170us/step - loss: 0.6613 - val_loss: 0.6662\n",
      "Epoch 60/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6612 - val_loss: 0.6659\n",
      "Epoch 61/120\n",
      "250/250 [==============================] - 0s 148us/step - loss: 0.6612 - val_loss: 0.6658\n",
      "Epoch 62/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6612 - val_loss: 0.6657\n",
      "Epoch 63/120\n",
      "250/250 [==============================] - 0s 145us/step - loss: 0.6611 - val_loss: 0.6659\n",
      "Epoch 64/120\n",
      "250/250 [==============================] - 0s 149us/step - loss: 0.6611 - val_loss: 0.6656\n",
      "Epoch 65/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6610 - val_loss: 0.6658\n",
      "Epoch 66/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6608 - val_loss: 0.6651\n",
      "Epoch 67/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6607 - val_loss: 0.6648\n",
      "Epoch 68/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6605 - val_loss: 0.6645\n",
      "Epoch 69/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6601 - val_loss: 0.6640\n",
      "Epoch 70/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6598 - val_loss: 0.6634\n",
      "Epoch 71/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6595 - val_loss: 0.6627\n",
      "Epoch 72/120\n",
      "250/250 [==============================] - 0s 190us/step - loss: 0.6590 - val_loss: 0.6617\n",
      "Epoch 73/120\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.658 - 0s 159us/step - loss: 0.6579 - val_loss: 0.6595\n",
      "Epoch 74/120\n",
      "250/250 [==============================] - 0s 172us/step - loss: 0.6577 - val_loss: 0.6586\n",
      "Epoch 75/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6568 - val_loss: 0.6548\n",
      "Epoch 76/120\n",
      "250/250 [==============================] - 0s 160us/step - loss: 0.6541 - val_loss: 0.6508\n",
      "Epoch 77/120\n",
      "250/250 [==============================] - 0s 170us/step - loss: 0.6525 - val_loss: 0.6533\n",
      "Epoch 78/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6579 - val_loss: 0.6601\n",
      "Epoch 79/120\n",
      "250/250 [==============================] - 0s 149us/step - loss: 0.6554 - val_loss: 0.6513\n",
      "Epoch 80/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6606 - val_loss: 0.6432\n",
      "Epoch 81/120\n",
      "250/250 [==============================] - 0s 155us/step - loss: 0.6464 - val_loss: 0.6464\n",
      "Epoch 82/120\n",
      "250/250 [==============================] - 0s 154us/step - loss: 0.6617 - val_loss: 0.6696\n",
      "Epoch 83/120\n",
      "250/250 [==============================] - 0s 160us/step - loss: 0.6623 - val_loss: 0.6438\n",
      "Epoch 84/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6470 - val_loss: 0.6477\n",
      "Epoch 85/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6532 - val_loss: 0.6421\n",
      "Epoch 86/120\n",
      "250/250 [==============================] - 0s 166us/step - loss: 0.6519 - val_loss: 0.6414\n",
      "Epoch 87/120\n",
      "250/250 [==============================] - 0s 160us/step - loss: 0.6533 - val_loss: 0.6397\n",
      "Epoch 88/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6468 - val_loss: 0.6401\n",
      "Epoch 89/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6573 - val_loss: 0.6415\n",
      "Epoch 90/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6544 - val_loss: 0.6407\n",
      "Epoch 91/120\n",
      "250/250 [==============================] - 0s 155us/step - loss: 0.6525 - val_loss: 0.6368\n",
      "Epoch 92/120\n",
      "250/250 [==============================] - 0s 154us/step - loss: 0.6443 - val_loss: 0.6437\n",
      "Epoch 93/120\n",
      "250/250 [==============================] - 0s 166us/step - loss: 0.6645 - val_loss: 0.6416\n",
      "Epoch 94/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6454 - val_loss: 0.6358\n",
      "Epoch 95/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6474 - val_loss: 0.6584\n",
      "Epoch 96/120\n",
      "250/250 [==============================] - 0s 165us/step - loss: 0.6521 - val_loss: 0.6417\n",
      "Epoch 97/120\n",
      "250/250 [==============================] - 0s 158us/step - loss: 0.6487 - val_loss: 0.6386\n",
      "Epoch 98/120\n",
      "250/250 [==============================] - 0s 154us/step - loss: 0.6524 - val_loss: 0.6346\n",
      "Epoch 99/120\n",
      "250/250 [==============================] - 0s 154us/step - loss: 0.6465 - val_loss: 0.6564\n",
      "Epoch 100/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6519 - val_loss: 0.6442\n",
      "Epoch 101/120\n",
      "250/250 [==============================] - 0s 160us/step - loss: 0.6475 - val_loss: 0.6335\n",
      "Epoch 102/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6460 - val_loss: 0.6379\n",
      "Epoch 103/120\n",
      "250/250 [==============================] - 0s 149us/step - loss: 0.6486 - val_loss: 0.6417\n",
      "Epoch 104/120\n",
      "250/250 [==============================] - 0s 161us/step - loss: 0.6463 - val_loss: 0.6340\n",
      "Epoch 105/120\n",
      "250/250 [==============================] - 0s 162us/step - loss: 0.6471 - val_loss: 0.6458\n",
      "Epoch 106/120\n",
      "250/250 [==============================] - 0s 171us/step - loss: 0.6451 - val_loss: 0.6303\n",
      "Epoch 107/120\n",
      "250/250 [==============================] - 0s 174us/step - loss: 0.6408 - val_loss: 0.6357\n",
      "Epoch 108/120\n",
      "250/250 [==============================] - 0s 157us/step - loss: 0.6517 - val_loss: 0.6424\n",
      "Epoch 109/120\n",
      "250/250 [==============================] - 0s 152us/step - loss: 0.6461 - val_loss: 0.6340\n",
      "Epoch 110/120\n",
      "250/250 [==============================] - 0s 173us/step - loss: 0.6506 - val_loss: 0.6370\n",
      "Epoch 111/120\n",
      "250/250 [==============================] - 0s 167us/step - loss: 0.6454 - val_loss: 0.6318\n",
      "Epoch 112/120\n",
      "250/250 [==============================] - 0s 168us/step - loss: 0.6444 - val_loss: 0.6406\n",
      "Epoch 113/120\n",
      "250/250 [==============================] - 0s 156us/step - loss: 0.6471 - val_loss: 0.6322\n",
      "Epoch 114/120\n",
      "250/250 [==============================] - 0s 177us/step - loss: 0.6453 - val_loss: 0.6397\n",
      "Epoch 115/120\n",
      "250/250 [==============================] - 0s 154us/step - loss: 0.6482 - val_loss: 0.6309\n",
      "Epoch 116/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6440 - val_loss: 0.6501\n",
      "Epoch 117/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6481 - val_loss: 0.6302\n",
      "Epoch 118/120\n",
      "250/250 [==============================] - 0s 164us/step - loss: 0.6434 - val_loss: 0.6379\n",
      "Epoch 119/120\n",
      "250/250 [==============================] - 0s 153us/step - loss: 0.6453 - val_loss: 0.6395\n",
      "Epoch 120/120\n",
      "250/250 [==============================] - 0s 147us/step - loss: 0.6591 - val_loss: 0.6317\n"
     ]
    }
   ],
   "source": [
    "from air_quality import trainAutoencoder\n",
    "# import importlib\n",
    "# importlib.reload(air_quality)\n",
    "\n",
    "encoder, decoder, autoencoder, history = trainAutoencoder(\n",
    "    train_data,\n",
    "    test_data, \n",
    "    val_data, \n",
    "    model_name = 'savedData/test.h5', \n",
    "    window_size = T,\n",
    "    batch_size = 100,\n",
    "    n_epochs = 120,\n",
    "    settings = settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
